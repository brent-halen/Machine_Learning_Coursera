{
  "name": "Machine learning coursera",
  "tagline": "Coursera Machine Learning Course Project",
  "body": "#Predicting Exercise Form from Accelerometer Data\r\n##A Project for Coursera's Machine Learning Course\r\n\r\n---\r\nauthor: \"Brent Halen\"\r\ndate: \"August 7, 2016\"\r\noutput: html_document\r\n---\r\n\r\nThe data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. \r\n\r\n###Background\r\n\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).\r\n\r\nThe training data for this project are available here:\r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n\r\nThe test data are available here:\r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\n```{r}\r\n## The following commented out line of code can be used to install the necessary packages if they're not already present on your computer. \r\n## install.packages(\"caret\",dep=T);install.packages(\"e1071\",dep=T);install.packages(\"randomForest\",dep=T);install.packages(\"gbm\",dep=T);install.packages(\"plyr\",dep=T);install.packages(\"MASS\",dep=T)\r\nlibrary(\"caret\")\r\nlibrary(\"e1071\")\r\nlibrary(\"randomForest\")\r\nlibrary(\"gbm\")\r\nlibrary(\"plyr\")\r\nlibrary(\"MASS\")\r\n```\r\n\r\n\r\n###Loading the Data\r\n\r\nFor this project, I pre-downloaded the files to my working directory. The files are called \"pml-training.csv\" and \"pml-testing.csv\". You can check whether or not these files are present with the list.files() command. \r\n\r\n```{r}\r\nlist.files()\r\n```\r\n\r\nIf the files aren't located in the working directory, you can get the working directory with the getwd() command and download the files to that directory. Otherwise, you can use the setwd() command to set the working directory to where you've downloaded the files. \r\n\r\n```{r}\r\ntraining <- read.csv('pml-training.csv')\r\nunknown_test <- as.data.frame(read.csv('pml-testing.csv'))\r\n```\r\n\r\n###Preprocessing\r\n\r\nAs we pre-process our training data, we'll apply the same changes to the unknown_test data. \r\n\r\n```{r}\r\ndim(training)\r\ndim(unknown_test)\r\n```\r\n\r\nTo simplify this dataset, we'll remove the columns with a high fraction of missing values (more than 90%). \r\n\r\n```{r}\r\ncount.NA <- sapply(training,function(x) sum(is.na(x)))\r\ncount.NA2 <- sapply(unknown_test,function(x) sum(is.na(x)))\r\nindex.NA <- c()\r\nindex.NA2 <- c()\r\nfor (i in 1:length(count.NA)) {\r\n  if (count.NA[[i]]/dim(training)[1] >= 0.9) {\r\n    index.NA <- append(index.NA,i)\r\n  }\r\n}\r\nfor (i in 1:length(count.NA2)) {\r\n  if (count.NA2[[i]]/dim(unknown_test)[1] >= 0.9) {\r\n    index.NA2 <- append(index.NA2,i)\r\n  }\r\n}\r\ntraining <- training[,-index.NA]\r\nunknown_test <- unknown_test[,-index.NA2]\r\nnearzero <- nearZeroVar(training,saveMetrics=T)\r\nnearzero2 <- nearZeroVar(unknown_test,saveMetrics=T)\r\nhead(nearzero)\r\ntraining <- training[,nearzero$nzv == FALSE]\r\nunknown_test <- unknown_test[,nearzero2$nzv == FALSE]\r\ndim(training);names(training)\r\n```\r\n\r\nWe don't need the observation ID, user_name, or the timestamps. We can remove those as well. \r\n\r\n```{r}\r\ntraining <- training[,-c(1:5)]; dim(training);\r\nunknown_test <- unknown_test[,-c(1:5)]; dim(unknown_test)\r\n```\r\n\r\nWe've simplified our dataset to 54 predictors. \r\n\r\n###Subsetting\r\n\r\nWe will partition the 'pml-training' data into a 'training' and 'test' set. \r\n\r\n```{r}\r\nset.seed(33414)\r\ninTrain <- createDataPartition(training$classe,p=0.75,list=FALSE)\r\nSubTrain <- training[inTrain,]\r\nSubTest <- training[-inTrain,]\r\n```\r\n\r\n###Training Models\r\n\r\nWe will train a random forest, boosted tree, discriminant analysis model, and a support vector machine. We will then stack the predictions of these models for a combined model using the random forest method. Then, we will analyze the accuracy of all 5 models and select the most effective one. This might take a while, so I would recommend anyone attempting to duplicate this at home do something else to occupy their time while it's fitting the models. This took my computer close to 2 hours to finish. \r\n\r\n```{r}\r\nset.seed(62433)\r\nmod_rf <- train(classe ~., data=SubTrain, method = \"rf\")\r\n```\r\n\r\n```{r}\r\nmod_gbm <- train(classe ~., data=SubTrain, method = \"gbm\",verbose=FALSE)\r\n```\r\n\r\n```{r}\r\nmod_lda <- train(classe ~., data=SubTrain, method = \"lda\")\r\n```\r\n\r\n```{r}\r\nmod_svm <- svm(classe ~ .,data=SubTrain)\r\n```\r\n\r\n```{r}\r\npred_rf <- predict(mod_rf, SubTest)\r\n```\r\n\r\n```{r}\r\npred_gbm <- predict(mod_gbm,SubTest)\r\n```\r\n\r\n```{r}\r\npred_lda <- predict(mod_lda,SubTest)\r\n```\r\n\r\n```{r}\r\npred_svm <- predict(mod_svm,SubTest)\r\n```\r\n\r\n###Accuracy Testing\r\n\r\nNow that our models are trained and we have our predictions made, we can test the accuracy. We will use 'confusionMatrix' to measure the accuracy of our models and compare their overall accuracy. \r\n\r\n#### Standard Random Forest\r\n\r\n```{r}\r\nconfusionMatrix(pred_rf, SubTest$classe)$overall[1]\r\n```\r\n\r\nThe standard Random Forest model has an accuracy rate of 0.997553. \r\n\r\n#### Boosted Trees\r\n\r\n```{r}\r\nconfusionMatrix(pred_gbm, SubTest$classe)$overall[1]\r\n```\r\n\r\nThe Boosted Trees model has an accuracy rate of 0.9857259. \r\n\r\n#### Linear Discriminant\r\n\r\n```{r}\r\nconfusionMatrix(pred_lda, SubTest$classe)$overall[1]\r\n```\r\n\r\nLinear Discriminant model has an accuracy rate of 0.7216558.\r\n\r\n#### Support Vector Machine\r\n\r\n```{r}\r\nconfusionMatrix(pred_svm, SubTest$classe)$overall[1]\r\n```\r\n\r\nThe support vector machine has an accuracy rate of 0.9502447. \r\n\r\n\r\n#### Select Best Model\r\n\r\nThe model with the highest accuracy was the random forest. Because of this, we will use the random forest going forward. \r\n\r\n```{r}\r\nplot(mod_rf)\r\n```\r\n\r\n```{r}\r\nplot(mod_rf$finalModel)\r\n```\r\n\r\n###Final Test Set\r\n\r\n#### Predictions on unknown sample\r\n\r\nIn this segment, we will make predictions on an unknown set of values provided in the \"pml-testing.csv\" file. This has already been assigned to the variable 'unknown_test'. \r\n\r\n```{r}\r\nfinalpredict <- predict(mod_rf, unknown_test)\r\nprint(finalpredict)\r\n```\r\n\r\n#### Save Output to file\r\n\r\nWe will now use the following code to output our predictions to the required file formats. \r\n\r\n```{r}\r\nanswer <- as.vector(finalpredict)\r\n\r\nwrite_answer_file = function(x) {\r\n    n = length(x)\r\n    for (i in 1:n) {\r\n        filename = paste0(\"problem_id_\", i, \".txt\")\r\n        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, \r\n            col.names = FALSE)\r\n    }\r\n}\r\n\r\nwrite_answer_file(answer)\r\n```\r\n\r\n### Conclusion\r\n\r\nI hope you've enjoyed this presentation and understand why I chose the models I did. Thank you for reading, and happy modeling. \r\n\r\n# References\r\n\r\n[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz4Gm71Ua1L",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}